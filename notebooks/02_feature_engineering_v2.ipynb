{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bafdd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FEATURE ENGINEERING FOR CUSTOMER SEGMENTATION\n",
      "============================================================\n",
      "\n",
      "üìÅ Path Configuration:\n",
      "  cwd: /workspaces/instacart-customer-clustering/notebooks\n",
      "  project_root: /workspaces/instacart-customer-clustering\n",
      "  src exists: True\n",
      "\n",
      "üì¶ Output directories:\n",
      "  features: /workspaces/instacart-customer-clustering/artifacts/features\n",
      "  figures: /workspaces/instacart-customer-clustering/artifacts/figures\n",
      "  models: /workspaces/instacart-customer-clustering/artifacts/models\n",
      "  reports: /workspaces/instacart-customer-clustering/artifacts/reports\n",
      "\n",
      "[1] Loading datasets via kagglehub...\n",
      "  data_dir: /home/codespace/.cache/kagglehub/datasets/psparks/instacart-market-basket-analysis/versions/1\n",
      "\n",
      "‚úì Loaded core datasets:\n",
      "  Orders: (3421083, 7)\n",
      "  Products: (49688, 4)\n",
      "  Aisles: (134, 2)\n",
      "  Departments: (21, 2)\n",
      "\n",
      "[2] Loading qualified users and filtering...\n",
      "  Qualified users: 182,223\n",
      "  Filtered orders: 3,325,139 (from 3,421,083)\n",
      "\n",
      "[3] Loading order products (memory-optimized)...\n",
      "  Relevant order IDs: 3,325,139\n",
      "  Loading order_products__prior.csv in chunks...\n",
      "    Processed 25,000,000 rows...\n",
      "  ‚úì Prior products: (31747215, 4)\n",
      "  Loading order_products__train.csv...\n",
      "  ‚úì Train products: (1234735, 4)\n",
      "\n",
      "‚úì Total order-product pairs: 32,981,950\n",
      "  Merging user_id into order_products...\n",
      "  ‚úì Order-products with user_id: (32981950, 5)\n",
      "\n",
      "[4] Creating product catalog...\n",
      "  ‚úì Products catalog: (49688, 6)\n",
      "\n",
      "[5] Initializing feature engineering...\n",
      "  Feature matrix initialized: (182223, 1)\n",
      "\n",
      "[6] Engineering Product Preference Features...\n",
      "------------------------------------------------------------\n",
      "  ‚Üí Merging product catalog with order products...\n",
      "    Merged shape: (32981950, 8)\n",
      "\n",
      "  ‚Üí Computing top 10 aisle purchase ratios...\n",
      "    Top 10 aisles: fresh fruits, fresh vegetables, packaged vegetables fruits...\n",
      "  ‚úì Created 10 aisle features\n",
      "\n",
      "  ‚Üí Computing department diversity...\n",
      "\n",
      "  ‚Üí Computing organic product preference...\n",
      "\n",
      "  ‚Üí Computing fresh food preference...\n",
      "  ‚úì Product preference features: (182223, 14)\n",
      "\n",
      "[7] Engineering Shopping Habit Features...\n",
      "------------------------------------------------------------\n",
      "\n",
      "  ‚Üí Computing order frequency metrics...\n",
      "\n",
      "  ‚Üí Computing basket size metrics...\n",
      "\n",
      "  ‚Üí Computing total order count...\n",
      "  ‚úì Shopping habit features: (182223, 21)\n",
      "\n",
      "[8] Engineering Temporal Pattern Features...\n",
      "------------------------------------------------------------\n",
      "\n",
      "  ‚Üí Computing hour preferences...\n",
      "\n",
      "  ‚Üí Computing weekend preference...\n",
      "\n",
      "  ‚Üí Computing day diversity...\n",
      "\n",
      "  ‚Üí Computing temporal consistency...\n",
      "  ‚úì Temporal features: (182223, 26)\n",
      "\n",
      "[9] Engineering Loyalty Indicator Features...\n",
      "------------------------------------------------------------\n",
      "\n",
      "  ‚Üí Computing reorder ratios...\n",
      "\n",
      "  ‚Üí Computing product variety...\n",
      "\n",
      "  ‚Üí Computing exploration score...\n",
      "\n",
      "  ‚Üí Computing repeat purchase metrics...\n",
      "  ‚úì Loyalty features: (182223, 31)\n",
      "\n",
      "[10] Handling Missing Values...\n",
      "------------------------------------------------------------\n",
      "  ‚ö†Ô∏è  Features with missing values: 10\n",
      "    aisle_bread: 1763\n",
      "    aisle_chips_pretzels: 1763\n",
      "    aisle_fresh_fruits: 1763\n",
      "    aisle_fresh_vegetables: 1763\n",
      "    aisle_milk: 1763\n",
      "    aisle_packaged_cheese: 1763\n",
      "    aisle_packaged_vegetables_fruits: 1763\n",
      "    aisle_soy_lactosefree: 1763\n",
      "    aisle_water_seltzer_sparkling_water: 1763\n",
      "    aisle_yogurt: 1763\n",
      "  ‚Üí Filling with 0...\n",
      "\n",
      "[11] Feature Scaling (Critical for Clustering)...\n",
      "------------------------------------------------------------\n",
      "\n",
      "  Features to scale: 30\n",
      "\n",
      "  ‚Üí Applying RobustScaler...\n",
      "\n",
      "  ‚úì Scaling verification:\n",
      "    Original mean range: [0.1560, 180.9977]\n",
      "    Scaled mean range: [-0.1131, 0.5934]\n",
      "    Original std range: [0.1478, 213.2544]\n",
      "    Scaled std range: [0.5321, 1.2847]\n",
      "\n",
      "  ‚úì Scaler saved to /workspaces/instacart-customer-clustering/artifacts/models/feature_scaler.pkl\n",
      "\n",
      "[12] Feature Variance Analysis...\n",
      "------------------------------------------------------------\n",
      "\n",
      "  Top 10 highest variance features:\n",
      "    1. total_product_instances: 1.6504\n",
      "    2. total_orders: 1.2901\n",
      "    3. aisle_soy_lactosefree: 0.9357\n",
      "    4. aisle_water_seltzer_sparkling_water: 0.8563\n",
      "    5. unique_products: 0.8244\n",
      "    6. aisle_chips_pretzels: 0.7551\n",
      "    7. aisle_bread: 0.7455\n",
      "    8. cv_order_frequency: 0.6985\n",
      "    9. cv_basket_size: 0.6745\n",
      "    10. avg_basket_size: 0.6623\n",
      "\n",
      "  Bottom 10 lowest variance features:\n",
      "    1. time_entropy: 0.4556\n",
      "    2. aisle_yogurt: 0.4425\n",
      "    3. reorder_ratio: 0.4402\n",
      "    4. repeat_purchase_rate: 0.4402\n",
      "    5. avg_days_between_orders: 0.4166\n",
      "    6. aisle_packaged_cheese: 0.4153\n",
      "    7. aisle_packaged_vegetables_fruits: 0.3583\n",
      "    8. organic_ratio: 0.3538\n",
      "    9. aisle_fresh_fruits: 0.3092\n",
      "    10. aisle_fresh_vegetables: 0.2832\n",
      "\n",
      "  ‚úì All features have sufficient variance\n",
      "\n",
      "[13] Saving Features...\n",
      "------------------------------------------------------------\n",
      "  ‚úì Unscaled features: user_features.csv (182223, 31)\n",
      "  ‚úì Scaled features: user_features_scaled.csv (182223, 31)\n",
      "  ‚úì Feature metadata: feature_metadata.csv\n",
      "  ‚úì Feature names: feature_names.csv\n",
      "\n",
      "[14] Creating Feature Description Table...\n",
      "------------------------------------------------------------\n",
      "  ‚úì Feature categories: feature_categories.json\n",
      "\n",
      "üì¶ Feature Categories:\n",
      "  aisle_features: 10\n",
      "  shopping_features: 7\n",
      "  temporal_features: 5\n",
      "  loyalty_features: 5\n",
      "  other_features: 2\n",
      "  Total: 30\n",
      "\n",
      "[15] Creating Visualizations...\n",
      "------------------------------------------------------------\n",
      "  ‚úì Saved: feature_distributions.png\n",
      "  ‚úì Saved: feature_correlations.png\n",
      "\n",
      "[16] Creating Summary Report...\n",
      "------------------------------------------------------------\n",
      "  ‚úì Summary report: /workspaces/instacart-customer-clustering/artifacts/reports/feature_engineering_summary.csv\n",
      "\n",
      "============================================================\n",
      "‚úÖ FEATURE ENGINEERING COMPLETE!\n",
      "============================================================\n",
      "\n",
      "üìä Summary:\n",
      "  Total users: 182,223\n",
      "  Total features: 30\n",
      "  Memory usage: 42.40 MB\n",
      "\n",
      "üìÅ Output Files:\n",
      "  /workspaces/instacart-customer-clustering/artifacts/features/user_features.csv\n",
      "  /workspaces/instacart-customer-clustering/artifacts/features/user_features_scaled.csv ‚Üê USE THIS FOR CLUSTERING\n",
      "  /workspaces/instacart-customer-clustering/artifacts/features/feature_metadata.csv\n",
      "  /workspaces/instacart-customer-clustering/artifacts/models/feature_scaler.pkl\n",
      "\n",
      "‚úÖ Features are properly scaled and ready for clustering!\n"
     ]
    }
   ],
   "source": [
    "# Instacart Customer Segmentation - Feature Engineering\n",
    "# Notebook 02: Creating User-Level Features for Clustering\n",
    "# Optimized for GitHub Codespaces + Kaggle Hub\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import gc\n",
    "import json\n",
    "import pickle\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE ENGINEERING FOR CUSTOMER SEGMENTATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# =============================================================================\n",
    "# 0. SETUP PATHS\n",
    "# =============================================================================\n",
    "project_root = Path.cwd()\n",
    "if project_root.name == \"notebooks\":\n",
    "    project_root = project_root.parent\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"\\nüìÅ Path Configuration:\")\n",
    "print(f\"  cwd: {Path.cwd()}\")\n",
    "print(f\"  project_root: {project_root}\")\n",
    "print(f\"  src exists: {(project_root / 'src').exists()}\")\n",
    "\n",
    "# Create artifact directories\n",
    "artifacts_dir = project_root / \"artifacts\"\n",
    "features_dir = artifacts_dir / \"features\"\n",
    "figures_dir = artifacts_dir / \"figures\"\n",
    "models_dir = artifacts_dir / \"models\"\n",
    "reports_dir = artifacts_dir / \"reports\"\n",
    "\n",
    "for d in [features_dir, figures_dir, models_dir, reports_dir]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nüì¶ Output directories:\")\n",
    "print(f\"  features: {features_dir}\")\n",
    "print(f\"  figures: {figures_dir}\")\n",
    "print(f\"  models: {models_dir}\")\n",
    "print(f\"  reports: {reports_dir}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. LOAD DATA VIA KAGGLE HUB\n",
    "# =============================================================================\n",
    "print(\"\\n[1] Loading datasets via kagglehub...\")\n",
    "\n",
    "from src.data.kaggle_download import download_instacart\n",
    "\n",
    "data_dir = Path(download_instacart())\n",
    "print(f\"  data_dir: {data_dir}\")\n",
    "\n",
    "def read_csv(name: str, usecols: list[str] | None = None, dtype: dict | None = None) -> pd.DataFrame:\n",
    "    \"\"\"Helper to read CSV with error handling\"\"\"\n",
    "    path = data_dir / name\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing file: {path}\")\n",
    "    return pd.read_csv(path, usecols=usecols, dtype=dtype)\n",
    "\n",
    "# Define dtypes for memory efficiency\n",
    "dtype_orders = {\n",
    "    \"order_id\": np.int32,\n",
    "    \"user_id\": np.int32,\n",
    "    \"eval_set\": \"category\",\n",
    "    \"order_number\": np.int16,\n",
    "    \"order_dow\": np.int8,\n",
    "    \"order_hour_of_day\": np.int8,\n",
    "    \"days_since_prior_order\": np.float32,\n",
    "}\n",
    "\n",
    "dtype_order_products = {\n",
    "    \"order_id\": np.int32,\n",
    "    \"product_id\": np.int32,\n",
    "    \"add_to_cart_order\": np.int16,\n",
    "    \"reordered\": np.int8,\n",
    "}\n",
    "\n",
    "dtype_products = {\n",
    "    \"product_id\": np.int32,\n",
    "    \"aisle_id\": np.int16,\n",
    "    \"department_id\": np.int16,\n",
    "}\n",
    "\n",
    "# Load main datasets\n",
    "orders = read_csv(\"orders.csv\", dtype=dtype_orders)\n",
    "products = read_csv(\"products.csv\", dtype=dtype_products)\n",
    "aisles = read_csv(\"aisles.csv\", dtype={\"aisle_id\": np.int16})\n",
    "departments = read_csv(\"departments.csv\", dtype={\"department_id\": np.int16})\n",
    "\n",
    "print(f\"\\n‚úì Loaded core datasets:\")\n",
    "print(f\"  Orders: {orders.shape}\")\n",
    "print(f\"  Products: {products.shape}\")\n",
    "print(f\"  Aisles: {aisles.shape}\")\n",
    "print(f\"  Departments: {departments.shape}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. LOAD QUALIFIED USERS & FILTER ORDERS\n",
    "# =============================================================================\n",
    "print(\"\\n[2] Loading qualified users and filtering...\")\n",
    "\n",
    "# Load qualified users from previous notebook\n",
    "qualified_users_path = features_dir / \"qualified_users.csv\"\n",
    "if not qualified_users_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Qualified users not found at {qualified_users_path}. \"\n",
    "        \"Please run 01_data_exploration.ipynb first!\"\n",
    "    )\n",
    "\n",
    "qualified_users = pd.read_csv(qualified_users_path, dtype={\"user_id\": np.int32})\n",
    "print(f\"  Qualified users: {len(qualified_users):,}\")\n",
    "\n",
    "# Filter orders to qualified users only (EARLY FILTERING = MEMORY SAVINGS)\n",
    "orders_f = orders[orders[\"user_id\"].isin(qualified_users[\"user_id\"])].copy()\n",
    "print(f\"  Filtered orders: {len(orders_f):,} (from {len(orders):,})\")\n",
    "\n",
    "# Free up memory\n",
    "del orders\n",
    "gc.collect()\n",
    "\n",
    "# =============================================================================\n",
    "# 3. LOAD ORDER PRODUCTS (CHUNKED & FILTERED)\n",
    "# =============================================================================\n",
    "print(\"\\n[3] Loading order products (memory-optimized)...\")\n",
    "\n",
    "# Get order IDs we care about\n",
    "relevant_order_ids = set(orders_f[\"order_id\"].values)\n",
    "print(f\"  Relevant order IDs: {len(relevant_order_ids):,}\")\n",
    "\n",
    "# Load prior orders in chunks\n",
    "print(\"  Loading order_products__prior.csv in chunks...\")\n",
    "chunks = []\n",
    "chunk_size = 5_000_000\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(\n",
    "    data_dir / \"order_products__prior.csv\",\n",
    "    dtype=dtype_order_products,\n",
    "    chunksize=chunk_size\n",
    ")):\n",
    "    # Filter to relevant orders only\n",
    "    chunk_filtered = chunk[chunk[\"order_id\"].isin(relevant_order_ids)].copy()\n",
    "    chunks.append(chunk_filtered)\n",
    "    \n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"    Processed {(i+1) * chunk_size:,} rows...\")\n",
    "    \n",
    "    del chunk\n",
    "    gc.collect()\n",
    "\n",
    "order_products_prior = pd.concat(chunks, ignore_index=True)\n",
    "del chunks\n",
    "gc.collect()\n",
    "\n",
    "print(f\"  ‚úì Prior products: {order_products_prior.shape}\")\n",
    "\n",
    "# Load train orders (smaller file)\n",
    "print(\"  Loading order_products__train.csv...\")\n",
    "order_products_train = read_csv(\"order_products__train.csv\", dtype=dtype_order_products)\n",
    "order_products_train = order_products_train[\n",
    "    order_products_train[\"order_id\"].isin(relevant_order_ids)\n",
    "].copy()\n",
    "print(f\"  ‚úì Train products: {order_products_train.shape}\")\n",
    "\n",
    "# Combine prior + train\n",
    "op = pd.concat([order_products_prior, order_products_train], ignore_index=True)\n",
    "del order_products_prior, order_products_train\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\n‚úì Total order-product pairs: {len(op):,}\")\n",
    "\n",
    "# Add user_id to order_products for faster aggregation\n",
    "print(\"  Merging user_id into order_products...\")\n",
    "op = op.merge(orders_f[[\"order_id\", \"user_id\"]], on=\"order_id\", how=\"left\")\n",
    "print(f\"  ‚úì Order-products with user_id: {op.shape}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. CREATE PRODUCT CATALOG\n",
    "# =============================================================================\n",
    "print(\"\\n[4] Creating product catalog...\")\n",
    "\n",
    "products_full = (\n",
    "    products\n",
    "    .merge(aisles, on=\"aisle_id\", how=\"left\")\n",
    "    .merge(departments, on=\"department_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "print(f\"  ‚úì Products catalog: {products_full.shape}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 5. INITIALIZE FEATURE DATAFRAME\n",
    "# =============================================================================\n",
    "print(\"\\n[5] Initializing feature engineering...\")\n",
    "\n",
    "user_features = pd.DataFrame({\"user_id\": qualified_users[\"user_id\"]})\n",
    "print(f\"  Feature matrix initialized: {user_features.shape}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 6. PRODUCT PREFERENCE FEATURES (OPTIMIZED - MINIMAL SET)\n",
    "# =============================================================================\n",
    "print(\"\\n[6] Engineering Product Preference Features...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Pre-merge product info with op ONCE (huge speed boost!)\n",
    "print(\"  ‚Üí Merging product catalog with order products...\")\n",
    "op_with_products = op.merge(\n",
    "    products_full[[\"product_id\", \"aisle\", \"department_id\", \"product_name\"]],\n",
    "    on=\"product_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "print(f\"    Merged shape: {op_with_products.shape}\")\n",
    "\n",
    "# 6.1 Top 10 Aisles Only (reduced from 30 for speed)\n",
    "print(\"\\n  ‚Üí Computing top 10 aisle purchase ratios...\")\n",
    "\n",
    "# Find top 10 most popular aisles by product count\n",
    "top_aisles = (\n",
    "    op_with_products[\"aisle\"]\n",
    "    .value_counts()\n",
    "    .head(10)\n",
    "    .index\n",
    "    .tolist()\n",
    ")\n",
    "print(f\"    Top 10 aisles: {', '.join(top_aisles[:3])}...\")\n",
    "\n",
    "# Filter to top aisles and aggregate\n",
    "op_top_aisles = op_with_products[op_with_products[\"aisle\"].isin(top_aisles)].copy()\n",
    "\n",
    "# Count orders per user per aisle (faster with filtered data)\n",
    "user_aisle_counts = (\n",
    "    op_top_aisles.groupby([\"user_id\", \"aisle\"])[\"order_id\"]\n",
    "    .nunique()\n",
    "    .reset_index(name=\"orders_with_aisle\")\n",
    ")\n",
    "\n",
    "# Total orders per user\n",
    "user_total_orders = (\n",
    "    orders_f.groupby(\"user_id\")[\"order_id\"]\n",
    "    .nunique()\n",
    "    .reset_index(name=\"total_orders\")\n",
    ")\n",
    "\n",
    "# Calculate ratios\n",
    "user_aisle_counts = user_aisle_counts.merge(user_total_orders, on=\"user_id\")\n",
    "user_aisle_counts[\"aisle_ratio\"] = (\n",
    "    user_aisle_counts[\"orders_with_aisle\"] / user_aisle_counts[\"total_orders\"]\n",
    ").astype(np.float32)\n",
    "\n",
    "# Pivot to wide format\n",
    "aisle_features = user_aisle_counts.pivot(\n",
    "    index=\"user_id\",\n",
    "    columns=\"aisle\",\n",
    "    values=\"aisle_ratio\"\n",
    ").fillna(0).reset_index()\n",
    "\n",
    "# Rename columns\n",
    "aisle_features.columns = [\"user_id\"] + [\n",
    "    f\"aisle_{col.lower().replace(' ', '_').replace('&', 'and')}\" \n",
    "    for col in aisle_features.columns[1:]\n",
    "]\n",
    "\n",
    "# Merge with main features\n",
    "user_features = user_features.merge(aisle_features, on=\"user_id\", how=\"left\")\n",
    "\n",
    "del user_aisle_counts, aisle_features, user_total_orders, op_top_aisles\n",
    "gc.collect()\n",
    "\n",
    "print(f\"  ‚úì Created {len([c for c in user_features.columns if c.startswith('aisle_')])} aisle features\")\n",
    "\n",
    "# 6.2 Department Diversity (already fast - keep as is)\n",
    "print(\"\\n  ‚Üí Computing department diversity...\")\n",
    "\n",
    "user_dept_diversity = (\n",
    "    op_with_products.groupby(\"user_id\")[\"department_id\"]\n",
    "    .nunique()\n",
    "    .reset_index(name=\"dept_diversity\")\n",
    "    .astype({\"dept_diversity\": np.int8})\n",
    ")\n",
    "\n",
    "user_features = user_features.merge(user_dept_diversity, on=\"user_id\", how=\"left\")\n",
    "del user_dept_diversity\n",
    "gc.collect()\n",
    "\n",
    "# 6.3 Organic Preference (ULTRA-OPTIMIZED!)\n",
    "print(\"\\n  ‚Üí Computing organic product preference...\")\n",
    "\n",
    "# Use already-merged data and vectorized operation\n",
    "op_with_products[\"is_organic\"] = (\n",
    "    op_with_products[\"product_name\"]\n",
    "    .str.contains(\"organic\", case=False, na=False)\n",
    "    .astype(np.int8)\n",
    ")\n",
    "\n",
    "# Direct aggregation on already-merged data\n",
    "user_organic = (\n",
    "    op_with_products.groupby(\"user_id\")[\"is_organic\"]\n",
    "    .mean()\n",
    "    .reset_index(name=\"organic_ratio\")\n",
    "    .astype({\"organic_ratio\": np.float32})\n",
    ")\n",
    "\n",
    "user_features = user_features.merge(user_organic, on=\"user_id\", how=\"left\")\n",
    "\n",
    "# 6.4 Fresh vs Packaged Ratio (bonus feature using same merged data)\n",
    "print(\"\\n  ‚Üí Computing fresh food preference...\")\n",
    "\n",
    "op_with_products[\"is_fresh\"] = (\n",
    "    op_with_products[\"aisle\"]\n",
    "    .str.contains(\"fresh\", case=False, na=False)\n",
    "    .astype(np.int8)\n",
    ")\n",
    "\n",
    "user_fresh = (\n",
    "    op_with_products.groupby(\"user_id\")[\"is_fresh\"]\n",
    "    .mean()\n",
    "    .reset_index(name=\"fresh_ratio\")\n",
    "    .astype({\"fresh_ratio\": np.float32})\n",
    ")\n",
    "\n",
    "user_features = user_features.merge(user_fresh, on=\"user_id\", how=\"left\")\n",
    "\n",
    "del user_organic, user_fresh, op_with_products\n",
    "gc.collect()\n",
    "\n",
    "print(f\"  ‚úì Product preference features: {user_features.shape}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 7. SHOPPING HABIT FEATURES\n",
    "# =============================================================================\n",
    "print(\"\\n[7] Engineering Shopping Habit Features...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# 7.1 Order Frequency\n",
    "print(\"\\n  ‚Üí Computing order frequency metrics...\")\n",
    "\n",
    "user_order_freq = (\n",
    "    orders_f[orders_f[\"days_since_prior_order\"].notna()]\n",
    "    .groupby(\"user_id\")[\"days_since_prior_order\"]\n",
    "    .agg([\n",
    "        (\"avg_days_between_orders\", \"mean\"),\n",
    "        (\"std_days_between_orders\", \"std\"),\n",
    "        (\"cv_order_frequency\", lambda x: x.std() / x.mean() if x.mean() > 0 else 0)\n",
    "    ])\n",
    "    .reset_index()\n",
    "    .astype({\n",
    "        \"avg_days_between_orders\": np.float32,\n",
    "        \"std_days_between_orders\": np.float32,\n",
    "        \"cv_order_frequency\": np.float32\n",
    "    })\n",
    ")\n",
    "\n",
    "user_features = user_features.merge(user_order_freq, on=\"user_id\", how=\"left\")\n",
    "\n",
    "# 7.2 Basket Size\n",
    "print(\"\\n  ‚Üí Computing basket size metrics...\")\n",
    "\n",
    "basket_size = (\n",
    "    op.groupby(\"order_id\")\n",
    "    .size()\n",
    "    .reset_index(name=\"basket_size\")\n",
    "    .astype({\"basket_size\": np.int16})\n",
    ")\n",
    "\n",
    "user_basket = (\n",
    "    orders_f.merge(basket_size, on=\"order_id\", how=\"left\")\n",
    "    .groupby(\"user_id\")[\"basket_size\"]\n",
    "    .agg([\n",
    "        (\"avg_basket_size\", \"mean\"),\n",
    "        (\"std_basket_size\", \"std\"),\n",
    "        (\"cv_basket_size\", lambda x: x.std() / x.mean() if x.mean() > 0 else 0)\n",
    "    ])\n",
    "    .reset_index()\n",
    "    .astype({\n",
    "        \"avg_basket_size\": np.float32,\n",
    "        \"std_basket_size\": np.float32,\n",
    "        \"cv_basket_size\": np.float32\n",
    "    })\n",
    ")\n",
    "\n",
    "user_features = user_features.merge(user_basket, on=\"user_id\", how=\"left\")\n",
    "\n",
    "del basket_size, user_order_freq, user_basket\n",
    "gc.collect()\n",
    "\n",
    "# 7.3 Total Orders\n",
    "print(\"\\n  ‚Üí Computing total order count...\")\n",
    "\n",
    "user_order_count = (\n",
    "    orders_f.groupby(\"user_id\")[\"order_id\"]\n",
    "    .count()\n",
    "    .reset_index(name=\"total_orders\")\n",
    "    .astype({\"total_orders\": np.int16})\n",
    ")\n",
    "\n",
    "user_features = user_features.merge(user_order_count, on=\"user_id\", how=\"left\")\n",
    "\n",
    "del user_order_count\n",
    "gc.collect()\n",
    "\n",
    "print(f\"  ‚úì Shopping habit features: {user_features.shape}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 8. TEMPORAL PATTERN FEATURES\n",
    "# =============================================================================\n",
    "print(\"\\n[8] Engineering Temporal Pattern Features...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# 8.1 Preferred Hour\n",
    "print(\"\\n  ‚Üí Computing hour preferences...\")\n",
    "\n",
    "user_hour = (\n",
    "    orders_f.groupby(\"user_id\")[\"order_hour_of_day\"]\n",
    "    .agg([\n",
    "        (\"avg_order_hour\", \"mean\"),\n",
    "        (\"std_order_hour\", \"std\")\n",
    "    ])\n",
    "    .reset_index()\n",
    "    .astype({\n",
    "        \"avg_order_hour\": np.float32,\n",
    "        \"std_order_hour\": np.float32\n",
    "    })\n",
    ")\n",
    "\n",
    "user_features = user_features.merge(user_hour, on=\"user_id\", how=\"left\")\n",
    "\n",
    "# 8.2 Weekend Preference\n",
    "print(\"\\n  ‚Üí Computing weekend preference...\")\n",
    "\n",
    "orders_f[\"is_weekend\"] = orders_f[\"order_dow\"].isin([0, 6]).astype(np.int8)\n",
    "\n",
    "user_weekend = (\n",
    "    orders_f.groupby(\"user_id\")[\"is_weekend\"]\n",
    "    .mean()\n",
    "    .reset_index(name=\"weekend_ratio\")\n",
    "    .astype({\"weekend_ratio\": np.float32})\n",
    ")\n",
    "\n",
    "user_features = user_features.merge(user_weekend, on=\"user_id\", how=\"left\")\n",
    "\n",
    "# 8.3 Day Diversity\n",
    "print(\"\\n  ‚Üí Computing day diversity...\")\n",
    "\n",
    "user_dow_diversity = (\n",
    "    orders_f.groupby(\"user_id\")[\"order_dow\"]\n",
    "    .nunique()\n",
    "    .reset_index(name=\"dow_diversity\")\n",
    "    .astype({\"dow_diversity\": np.int8})\n",
    ")\n",
    "\n",
    "user_features = user_features.merge(user_dow_diversity, on=\"user_id\", how=\"left\")\n",
    "\n",
    "# 8.4 Temporal Entropy\n",
    "print(\"\\n  ‚Üí Computing temporal consistency...\")\n",
    "\n",
    "def temporal_entropy(hours):\n",
    "    \"\"\"Calculate entropy of ordering hours\"\"\"\n",
    "    if len(hours) < 2:\n",
    "        return 0.0\n",
    "    hour_dist = pd.Series(hours).value_counts(normalize=True)\n",
    "    return float(stats.entropy(hour_dist))\n",
    "\n",
    "user_time_entropy = (\n",
    "    orders_f.groupby(\"user_id\")[\"order_hour_of_day\"]\n",
    "    .apply(temporal_entropy)\n",
    "    .reset_index(name=\"time_entropy\")\n",
    "    .astype({\"time_entropy\": np.float32})\n",
    ")\n",
    "\n",
    "user_features = user_features.merge(user_time_entropy, on=\"user_id\", how=\"left\")\n",
    "\n",
    "del user_hour, user_weekend, user_dow_diversity, user_time_entropy\n",
    "gc.collect()\n",
    "\n",
    "print(f\"  ‚úì Temporal features: {user_features.shape}\")\n",
    "\n",
    "# NOW we can delete orders_f (no longer needed)\n",
    "del orders_f\n",
    "gc.collect()\n",
    "\n",
    "# =============================================================================\n",
    "# 9. LOYALTY INDICATOR FEATURES\n",
    "# =============================================================================\n",
    "print(\"\\n[9] Engineering Loyalty Indicator Features...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# 9.1 Reorder Ratio\n",
    "print(\"\\n  ‚Üí Computing reorder ratios...\")\n",
    "\n",
    "user_reorder = (\n",
    "    op.groupby(\"user_id\")[\"reordered\"]\n",
    "    .mean()\n",
    "    .reset_index(name=\"reorder_ratio\")\n",
    "    .astype({\"reorder_ratio\": np.float32})\n",
    ")\n",
    "\n",
    "user_features = user_features.merge(user_reorder, on=\"user_id\", how=\"left\")\n",
    "\n",
    "# 9.2 Product Variety\n",
    "print(\"\\n  ‚Üí Computing product variety...\")\n",
    "\n",
    "user_product_variety = (\n",
    "    op.groupby(\"user_id\")[\"product_id\"]\n",
    "    .nunique()\n",
    "    .reset_index(name=\"unique_products\")\n",
    "    .astype({\"unique_products\": np.int16})\n",
    ")\n",
    "\n",
    "user_features = user_features.merge(user_product_variety, on=\"user_id\", how=\"left\")\n",
    "\n",
    "# 9.3 Exploration Metric\n",
    "print(\"\\n  ‚Üí Computing exploration score...\")\n",
    "\n",
    "user_features[\"products_per_order\"] = (\n",
    "    user_features[\"unique_products\"] / user_features[\"total_orders\"]\n",
    ").astype(np.float32)\n",
    "\n",
    "# 9.4 Repeat Purchase Rate\n",
    "print(\"\\n  ‚Üí Computing repeat purchase metrics...\")\n",
    "\n",
    "total_product_instances = (\n",
    "    op.groupby(\"user_id\")[\"product_id\"]\n",
    "    .count()\n",
    "    .reset_index(name=\"total_product_instances\")\n",
    "    .astype({\"total_product_instances\": np.int32})\n",
    ")\n",
    "\n",
    "user_features = user_features.merge(total_product_instances, on=\"user_id\", how=\"left\")\n",
    "\n",
    "user_features[\"repeat_purchase_rate\"] = (\n",
    "    (user_features[\"total_product_instances\"] - user_features[\"unique_products\"]) /\n",
    "    user_features[\"total_product_instances\"]\n",
    ").astype(np.float32)\n",
    "\n",
    "del user_reorder, user_product_variety, total_product_instances\n",
    "gc.collect()\n",
    "\n",
    "print(f\"  ‚úì Loyalty features: {user_features.shape}\")\n",
    "\n",
    "# Cleanup big dataframes (keep orders_f for now, we need it!)\n",
    "del op\n",
    "gc.collect()\n",
    "\n",
    "# =============================================================================\n",
    "# 10. HANDLE MISSING VALUES\n",
    "# =============================================================================\n",
    "print(\"\\n[10] Handling Missing Values...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "missing_counts = user_features.isnull().sum()\n",
    "missing_features = missing_counts[missing_counts > 0]\n",
    "\n",
    "if len(missing_features) > 0:\n",
    "    print(f\"  ‚ö†Ô∏è  Features with missing values: {len(missing_features)}\")\n",
    "    for feat, count in missing_features.items():\n",
    "        print(f\"    {feat}: {count}\")\n",
    "    print(\"  ‚Üí Filling with 0...\")\n",
    "    user_features = user_features.fillna(0)\n",
    "else:\n",
    "    print(\"  ‚úì No missing values!\")\n",
    "\n",
    "# =============================================================================\n",
    "# 11. FEATURE SCALING\n",
    "# =============================================================================\n",
    "print(\"\\n[11] Feature Scaling (Critical for Clustering)...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "feature_cols = [col for col in user_features.columns if col != \"user_id\"]\n",
    "print(f\"\\n  Features to scale: {len(feature_cols)}\")\n",
    "\n",
    "# Create scaled version\n",
    "user_features_scaled = user_features[[\"user_id\"]].copy()\n",
    "\n",
    "# Use RobustScaler (resistant to outliers)\n",
    "print(\"\\n  ‚Üí Applying RobustScaler...\")\n",
    "scaler = RobustScaler()\n",
    "\n",
    "scaled_values = scaler.fit_transform(user_features[feature_cols])\n",
    "scaled_df = pd.DataFrame(\n",
    "    scaled_values,\n",
    "    columns=feature_cols,\n",
    "    index=user_features.index\n",
    ")\n",
    "\n",
    "user_features_scaled = pd.concat([user_features_scaled, scaled_df], axis=1)\n",
    "\n",
    "# Verify scaling\n",
    "print(\"\\n  ‚úì Scaling verification:\")\n",
    "print(f\"    Original mean range: [{user_features[feature_cols].mean().min():.4f}, \"\n",
    "      f\"{user_features[feature_cols].mean().max():.4f}]\")\n",
    "print(f\"    Scaled mean range: [{user_features_scaled[feature_cols].mean().min():.4f}, \"\n",
    "      f\"{user_features_scaled[feature_cols].mean().max():.4f}]\")\n",
    "print(f\"    Original std range: [{user_features[feature_cols].std().min():.4f}, \"\n",
    "      f\"{user_features[feature_cols].std().max():.4f}]\")\n",
    "print(f\"    Scaled std range: [{user_features_scaled[feature_cols].std().min():.4f}, \"\n",
    "      f\"{user_features_scaled[feature_cols].std().max():.4f}]\")\n",
    "\n",
    "# Save scaler\n",
    "scaler_path = models_dir / \"feature_scaler.pkl\"\n",
    "with open(scaler_path, \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"\\n  ‚úì Scaler saved to {scaler_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 12. FEATURE VARIANCE ANALYSIS\n",
    "# =============================================================================\n",
    "print(\"\\n[12] Feature Variance Analysis...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "feature_variance = user_features_scaled[feature_cols].var().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n  Top 10 highest variance features:\")\n",
    "for i, (feat, var) in enumerate(feature_variance.head(10).items(), 1):\n",
    "    print(f\"    {i}. {feat}: {var:.4f}\")\n",
    "\n",
    "print(\"\\n  Bottom 10 lowest variance features:\")\n",
    "for i, (feat, var) in enumerate(feature_variance.tail(10).items(), 1):\n",
    "    print(f\"    {i}. {feat}: {var:.4f}\")\n",
    "\n",
    "# Flag low-variance features\n",
    "low_var_threshold = 0.01\n",
    "low_var_features = feature_variance[feature_variance < low_var_threshold].index.tolist()\n",
    "\n",
    "if len(low_var_features) > 0:\n",
    "    print(f\"\\n  ‚ö†Ô∏è  {len(low_var_features)} features have very low variance (< {low_var_threshold})\")\n",
    "else:\n",
    "    print(\"\\n  ‚úì All features have sufficient variance\")\n",
    "\n",
    "# =============================================================================\n",
    "# 13. SAVE FEATURES\n",
    "# =============================================================================\n",
    "print(\"\\n[13] Saving Features...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Save unscaled\n",
    "unscaled_path = features_dir / \"user_features.csv\"\n",
    "user_features.to_csv(unscaled_path, index=False)\n",
    "print(f\"  ‚úì Unscaled features: {unscaled_path.name} {user_features.shape}\")\n",
    "\n",
    "# Save scaled (FOR CLUSTERING!)\n",
    "scaled_path = features_dir / \"user_features_scaled.csv\"\n",
    "user_features_scaled.to_csv(scaled_path, index=False)\n",
    "print(f\"  ‚úì Scaled features: {scaled_path.name} {user_features_scaled.shape}\")\n",
    "\n",
    "# Save feature metadata\n",
    "feature_metadata = pd.DataFrame({\n",
    "    \"feature_name\": feature_cols,\n",
    "    \"variance_scaled\": [feature_variance.get(f, 0) for f in feature_cols],\n",
    "    \"mean_original\": user_features[feature_cols].mean().values,\n",
    "    \"std_original\": user_features[feature_cols].std().values,\n",
    "    \"mean_scaled\": user_features_scaled[feature_cols].mean().values,\n",
    "    \"std_scaled\": user_features_scaled[feature_cols].std().values,\n",
    "})\n",
    "\n",
    "metadata_path = features_dir / \"feature_metadata.csv\"\n",
    "feature_metadata.to_csv(metadata_path, index=False)\n",
    "print(f\"  ‚úì Feature metadata: {metadata_path.name}\")\n",
    "\n",
    "# Save feature names\n",
    "feature_names_path = features_dir / \"feature_names.csv\"\n",
    "pd.DataFrame({\"feature_name\": feature_cols}).to_csv(feature_names_path, index=False)\n",
    "print(f\"  ‚úì Feature names: {feature_names_path.name}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 14. CREATE FEATURE DESCRIPTION TABLE\n",
    "# =============================================================================\n",
    "print(\"\\n[14] Creating Feature Description Table...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "aisle_cols = [col for col in feature_cols if col.startswith(\"aisle_\")]\n",
    "other_cols = [col for col in feature_cols if not col.startswith(\"aisle_\")]\n",
    "\n",
    "feature_categories = {\n",
    "    \"aisle_features\": aisle_cols,\n",
    "    \"shopping_features\": [\n",
    "        \"avg_days_between_orders\", \"std_days_between_orders\", \"cv_order_frequency\",\n",
    "        \"avg_basket_size\", \"std_basket_size\", \"cv_basket_size\", \"total_orders\"\n",
    "    ],\n",
    "    \"temporal_features\": [\n",
    "        \"avg_order_hour\", \"std_order_hour\", \"weekend_ratio\",\n",
    "        \"dow_diversity\", \"time_entropy\"\n",
    "    ],\n",
    "    \"loyalty_features\": [\n",
    "        \"reorder_ratio\", \"unique_products\", \"products_per_order\",\n",
    "        \"total_product_instances\", \"repeat_purchase_rate\"\n",
    "    ],\n",
    "    \"other_features\": [\"dept_diversity\", \"organic_ratio\"]\n",
    "}\n",
    "\n",
    "categories_path = features_dir / \"feature_categories.json\"\n",
    "with open(categories_path, \"w\") as f:\n",
    "    json.dump(feature_categories, f, indent=2)\n",
    "print(f\"  ‚úì Feature categories: {categories_path.name}\")\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nüì¶ Feature Categories:\")\n",
    "for cat_name, cat_features in feature_categories.items():\n",
    "    print(f\"  {cat_name}: {len(cat_features)}\")\n",
    "print(f\"  Total: {len(feature_cols)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 15. VISUALIZATIONS\n",
    "# =============================================================================\n",
    "print(\"\\n[15] Creating Visualizations...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Select key features to visualize\n",
    "key_features = [\n",
    "    \"avg_days_between_orders\",\n",
    "    \"avg_basket_size\",\n",
    "    \"dept_diversity\",\n",
    "    \"reorder_ratio\",\n",
    "    \"organic_ratio\",\n",
    "    \"weekend_ratio\",\n",
    "    \"avg_order_hour\",\n",
    "    \"unique_products\"\n",
    "]\n",
    "\n",
    "# Feature distributions\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(key_features):\n",
    "    if feature in user_features.columns:\n",
    "        axes[idx].hist(\n",
    "            user_features[feature],\n",
    "            bins=50,\n",
    "            edgecolor=\"black\",\n",
    "            alpha=0.7,\n",
    "            color=\"steelblue\"\n",
    "        )\n",
    "        axes[idx].set_title(\n",
    "            feature.replace(\"_\", \" \").title(),\n",
    "            fontsize=10,\n",
    "            fontweight=\"bold\"\n",
    "        )\n",
    "        axes[idx].set_xlabel(\"Value\")\n",
    "        axes[idx].set_ylabel(\"Frequency\")\n",
    "        axes[idx].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "        mean_val = user_features[feature].mean()\n",
    "        axes[idx].axvline(\n",
    "            mean_val,\n",
    "            color=\"red\",\n",
    "            linestyle=\"--\",\n",
    "            label=f\"Mean: {mean_val:.2f}\",\n",
    "            linewidth=2\n",
    "        )\n",
    "        axes[idx].legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "dist_path = figures_dir / \"feature_distributions.png\"\n",
    "plt.savefig(dist_path, dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"  ‚úì Saved: {dist_path.name}\")\n",
    "plt.close()\n",
    "\n",
    "# Correlation heatmap\n",
    "corr_matrix = user_features[key_features].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    square=True,\n",
    "    linewidths=1,\n",
    "    cbar_kws={\"shrink\": 0.8}\n",
    ")\n",
    "plt.title(\n",
    "    \"Feature Correlation Matrix (Key Features)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    "    pad=20\n",
    ")\n",
    "plt.tight_layout()\n",
    "corr_path = figures_dir / \"feature_correlations.png\"\n",
    "plt.savefig(corr_path, dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"  ‚úì Saved: {corr_path.name}\")\n",
    "plt.close()\n",
    "\n",
    "# =============================================================================\n",
    "# 16. SUMMARY REPORT\n",
    "# =============================================================================\n",
    "print(\"\\n[16] Creating Summary Report...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "summary_report = {\n",
    "    \"total_users\": len(user_features),\n",
    "    \"total_features\": len(feature_cols),\n",
    "    \"aisle_features\": len(aisle_cols),\n",
    "    \"shopping_features\": len(feature_categories[\"shopping_features\"]),\n",
    "    \"temporal_features\": len(feature_categories[\"temporal_features\"]),\n",
    "    \"loyalty_features\": len(feature_categories[\"loyalty_features\"]),\n",
    "    \"other_features\": len(feature_categories[\"other_features\"]),\n",
    "    \"missing_values\": 0,\n",
    "    \"scaling_method\": \"RobustScaler\",\n",
    "    \"date_created\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "\n",
    "summary_path = reports_dir / \"feature_engineering_summary.csv\"\n",
    "pd.DataFrame([summary_report]).T.to_csv(summary_path, header=[\"value\"])\n",
    "print(f\"  ‚úì Summary report: {summary_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# COMPLETE!\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ FEATURE ENGINEERING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"  Total users: {len(user_features):,}\")\n",
    "print(f\"  Total features: {len(feature_cols)}\")\n",
    "print(f\"  Memory usage: {user_features_scaled.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"\\nüìÅ Output Files:\")\n",
    "print(f\"  {unscaled_path}\")\n",
    "print(f\"  {scaled_path} ‚Üê USE THIS FOR CLUSTERING\")\n",
    "print(f\"  {metadata_path}\")\n",
    "print(f\"  {scaler_path}\")\n",
    "print(f\"\\n‚úÖ Features are properly scaled and ready for clustering!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
