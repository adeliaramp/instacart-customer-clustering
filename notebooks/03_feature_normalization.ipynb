{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e5f3f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ADVANCED FEATURE NORMALIZATION - FIXING DOMINANCE ISSUES\n",
      "================================================================================\n",
      "\n",
      "[1] Loading features...\n",
      "  Features: 30\n",
      "\n",
      "[2] Analyzing scaling quality...\n",
      "\n",
      "  Current scaled statistics:\n",
      "    Mean range: [-0.1131, 0.5934]\n",
      "    Std range: [0.5321, 1.2847]\n",
      "    Variance range: [0.2832, 1.6504]\n",
      "\n",
      "  ‚ö†Ô∏è  Variance ratio (max/min): 5.83x\n",
      "  ‚ö†Ô∏è  HIGH VARIANCE INEQUALITY! Features will dominate clustering.\n",
      "  ‚ö†Ô∏è  Recommended: < 3x, Current: 5.83x\n",
      "\n",
      "  Features with excessive variance (>1.08):\n",
      "    total_orders: 1.2901\n",
      "    total_product_instances: 1.6504\n",
      "\n",
      "[3] Applying variance stabilization transformations...\n",
      "\n",
      "  ‚Üí Applying transformations:\n",
      "    Log transform: 3 features\n",
      "    Power transform: 12 features\n",
      "    Standard scaling: 30 features (final)\n",
      "      Log-transformed: total_product_instances\n",
      "      Log-transformed: total_orders\n",
      "      Log-transformed: unique_products\n",
      "      Power-transformed: 12 features\n",
      "\n",
      "  ‚Üí Applying StandardScaler for equal variance...\n",
      "\n",
      "[4] Verifying normalization quality...\n",
      "\n",
      "  NEW scaled statistics:\n",
      "    Mean range: [-0.0000, 0.0000]\n",
      "    Std range: [1.0000, 1.0000]\n",
      "    Variance range: [1.0000, 1.0000]\n",
      "\n",
      "  ‚úÖ NEW variance ratio: 1.00x\n",
      "  ‚úÖ Improvement: 5.83x ‚Üí 1.00x\n",
      "  ‚úÖ GOOD! Variance inequality is now acceptable.\n",
      "\n",
      "[5] Creating comparison visualizations...\n",
      "  ‚úì Saved: variance_normalization_comparison.png\n",
      "  ‚úì Saved: feature_variance_detailed_comparison.png\n",
      "\n",
      "[6] Checking feature correlations...\n",
      "\n",
      "  ‚ö†Ô∏è  Found 3 highly correlated feature pairs (>0.8):\n",
      "    dept_diversity ‚Üî unique_products: 0.873\n",
      "    reorder_ratio ‚Üî repeat_purchase_rate: 1.000\n",
      "    unique_products ‚Üî total_product_instances: 0.907\n",
      "  üí° Consider removing one from each pair to reduce redundancy.\n",
      "\n",
      "[7] Saving normalized features...\n",
      "  ‚úì Normalized features: user_features_normalized.csv\n",
      "  ‚úì Pipeline: normalization_pipeline.pkl\n",
      "  ‚úì Report: variance_comparison_report.csv\n",
      "\n",
      "================================================================================\n",
      "‚úÖ FEATURE NORMALIZATION COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "üìä Summary:\n",
      "  Variance ratio improvement: 5.83x ‚Üí 1.00x\n",
      "  Mean centering: 0.0000 (closer to 0 is better)\n",
      "  Std deviation: 1.0000 - 1.0000\n",
      "\n",
      "üìÅ Files Created:\n",
      "  /workspaces/instacart-customer-clustering/artifacts/features/user_features_normalized.csv\n",
      "  /workspaces/instacart-customer-clustering/artifacts/models/normalization_pipeline.pkl\n",
      "  /workspaces/instacart-customer-clustering/artifacts/figures/variance_normalization_comparison.png\n",
      "  /workspaces/instacart-customer-clustering/artifacts/figures/feature_variance_detailed_comparison.png\n",
      "\n",
      "üéØ RECOMMENDATION:\n",
      "  ‚úÖ USE: user_features_normalized.csv for clustering\n",
      "  ‚úÖ Features are well-balanced and ready for K-Means/DBSCAN\n",
      "\n",
      "üí° For K-Means: Use user_features_normalized.csv\n",
      "üí° For DBSCAN: Test both (DBSCAN is more scale-sensitive)\n",
      "üí° For Hierarchical: Normalized version is preferred\n",
      "\n",
      "[OPTIONAL] Removing redundant features...\n",
      "  Features to drop (redundant): {'repeat_purchase_rate', 'unique_products', 'dept_diversity'}\n",
      "  ‚úì Reduced features: user_features_reduced.csv\n",
      "  ‚úì Reduced from 30 ‚Üí 27 features\n"
     ]
    }
   ],
   "source": [
    "# Additional Feature Engineering Steps - Fix Feature Dominance\n",
    "# Run this AFTER your current feature engineering completes\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, PowerTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ADVANCED FEATURE NORMALIZATION - FIXING DOMINANCE ISSUES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# SETUP\n",
    "# =============================================================================\n",
    "project_root = Path(\"/workspaces/instacart-customer-clustering\")\n",
    "features_dir = project_root / \"artifacts\" / \"features\"\n",
    "models_dir = project_root / \"artifacts\" / \"models\"\n",
    "figures_dir = project_root / \"artifacts\" / \"figures\"\n",
    "\n",
    "# =============================================================================\n",
    "# 1. LOAD FEATURES\n",
    "# =============================================================================\n",
    "print(\"\\n[1] Loading features...\")\n",
    "\n",
    "user_features = pd.read_csv(features_dir / \"user_features.csv\")\n",
    "user_features_scaled = pd.read_csv(features_dir / \"user_features_scaled.csv\")\n",
    "\n",
    "feature_cols = [col for col in user_features.columns if col != \"user_id\"]\n",
    "print(f\"  Features: {len(feature_cols)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. ANALYZE CURRENT SCALING ISSUES\n",
    "# =============================================================================\n",
    "print(\"\\n[2] Analyzing scaling quality...\")\n",
    "\n",
    "scaled_variance = user_features_scaled[feature_cols].var()\n",
    "scaled_mean = user_features_scaled[feature_cols].mean()\n",
    "scaled_std = user_features_scaled[feature_cols].std()\n",
    "\n",
    "print(f\"\\n  Current scaled statistics:\")\n",
    "print(f\"    Mean range: [{scaled_mean.min():.4f}, {scaled_mean.max():.4f}]\")\n",
    "print(f\"    Std range: [{scaled_std.min():.4f}, {scaled_std.max():.4f}]\")\n",
    "print(f\"    Variance range: [{scaled_variance.min():.4f}, {scaled_variance.max():.4f}]\")\n",
    "\n",
    "# Check for dominance\n",
    "variance_ratio = scaled_variance.max() / scaled_variance.min()\n",
    "print(f\"\\n  ‚ö†Ô∏è  Variance ratio (max/min): {variance_ratio:.2f}x\")\n",
    "\n",
    "if variance_ratio > 3:\n",
    "    print(f\"  ‚ö†Ô∏è  HIGH VARIANCE INEQUALITY! Features will dominate clustering.\")\n",
    "    print(f\"  ‚ö†Ô∏è  Recommended: < 3x, Current: {variance_ratio:.2f}x\")\n",
    "\n",
    "# Identify problematic features\n",
    "high_var_features = scaled_variance[scaled_variance > scaled_variance.median() * 2]\n",
    "print(f\"\\n  Features with excessive variance (>{scaled_variance.median()*2:.2f}):\")\n",
    "for feat, var in high_var_features.items():\n",
    "    print(f\"    {feat}: {var:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3. APPLY VARIANCE STABILIZATION\n",
    "# =============================================================================\n",
    "print(\"\\n[3] Applying variance stabilization transformations...\")\n",
    "\n",
    "# Strategy: Multi-step normalization\n",
    "# Step 1: Log transform for heavy-tailed features\n",
    "# Step 2: Power transform for skewed features  \n",
    "# Step 3: Standard scaling to unit variance\n",
    "\n",
    "# Identify features needing log transform (count features)\n",
    "log_transform_features = [\n",
    "    'total_product_instances',\n",
    "    'total_orders',\n",
    "    'unique_products'\n",
    "]\n",
    "\n",
    "# Identify features needing power transform (ratio features)\n",
    "power_transform_features = [col for col in feature_cols \n",
    "                             if col.startswith('aisle_') or \n",
    "                             col in ['organic_ratio', 'fresh_ratio']]\n",
    "\n",
    "# Copy original features\n",
    "user_features_normalized = user_features[[\"user_id\"]].copy()\n",
    "\n",
    "print(f\"\\n  ‚Üí Applying transformations:\")\n",
    "print(f\"    Log transform: {len(log_transform_features)} features\")\n",
    "print(f\"    Power transform: {len(power_transform_features)} features\")\n",
    "print(f\"    Standard scaling: {len(feature_cols)} features (final)\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRANSFORMATION PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "transformed_features = user_features[feature_cols].copy()\n",
    "\n",
    "# Step 1: Log transform count features (stabilizes variance)\n",
    "for feat in log_transform_features:\n",
    "    if feat in transformed_features.columns:\n",
    "        # Add 1 to avoid log(0)\n",
    "        transformed_features[feat] = np.log1p(transformed_features[feat])\n",
    "        print(f\"      Log-transformed: {feat}\")\n",
    "\n",
    "# Step 2: Power transform ratio/aisle features (fixes skewness)\n",
    "pt = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "for feat in power_transform_features:\n",
    "    if feat in transformed_features.columns:\n",
    "        # Reshape for sklearn\n",
    "        values = transformed_features[[feat]].values\n",
    "        transformed_features[feat] = pt.fit_transform(values).flatten()\n",
    "\n",
    "print(f\"      Power-transformed: {len(power_transform_features)} features\")\n",
    "\n",
    "# Step 3: StandardScaler to unit variance (critical!)\n",
    "print(f\"\\n  ‚Üí Applying StandardScaler for equal variance...\")\n",
    "scaler_std = StandardScaler()\n",
    "scaled_values = scaler_std.fit_transform(transformed_features)\n",
    "\n",
    "normalized_df = pd.DataFrame(\n",
    "    scaled_values,\n",
    "    columns=feature_cols,\n",
    "    index=user_features.index\n",
    ")\n",
    "\n",
    "user_features_normalized = pd.concat([user_features_normalized, normalized_df], axis=1)\n",
    "\n",
    "# =============================================================================\n",
    "# 4. VERIFY IMPROVEMENTS\n",
    "# =============================================================================\n",
    "print(\"\\n[4] Verifying normalization quality...\")\n",
    "\n",
    "new_variance = normalized_df.var()\n",
    "new_mean = normalized_df.mean()\n",
    "new_std = normalized_df.std()\n",
    "\n",
    "print(f\"\\n  NEW scaled statistics:\")\n",
    "print(f\"    Mean range: [{new_mean.min():.4f}, {new_mean.max():.4f}]\")\n",
    "print(f\"    Std range: [{new_std.min():.4f}, {new_std.max():.4f}]\")\n",
    "print(f\"    Variance range: [{new_variance.min():.4f}, {new_variance.max():.4f}]\")\n",
    "\n",
    "new_variance_ratio = new_variance.max() / new_variance.min()\n",
    "print(f\"\\n  ‚úÖ NEW variance ratio: {new_variance_ratio:.2f}x\")\n",
    "print(f\"  ‚úÖ Improvement: {variance_ratio:.2f}x ‚Üí {new_variance_ratio:.2f}x\")\n",
    "\n",
    "if new_variance_ratio < 3:\n",
    "    print(f\"  ‚úÖ GOOD! Variance inequality is now acceptable.\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è  Still high. Consider removing outliers or additional transforms.\")\n",
    "\n",
    "# =============================================================================\n",
    "# 5. SIDE-BY-SIDE COMPARISON\n",
    "# =============================================================================\n",
    "print(\"\\n[5] Creating comparison visualizations...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Before\n",
    "axes[0].bar(range(len(scaled_variance)), scaled_variance.sort_values(ascending=False))\n",
    "axes[0].set_title(\"BEFORE: Feature Variance (Unequal)\", fontweight='bold', fontsize=14)\n",
    "axes[0].set_xlabel(\"Feature Rank\")\n",
    "axes[0].set_ylabel(\"Variance\")\n",
    "axes[0].axhline(y=1.0, color='r', linestyle='--', label='Target Variance')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# After\n",
    "axes[1].bar(range(len(new_variance)), new_variance.sort_values(ascending=False))\n",
    "axes[1].set_title(\"AFTER: Feature Variance (Normalized)\", fontweight='bold', fontsize=14)\n",
    "axes[1].set_xlabel(\"Feature Rank\")\n",
    "axes[1].set_ylabel(\"Variance\")\n",
    "axes[1].axhline(y=1.0, color='r', linestyle='--', label='Target Variance')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "variance_comparison_path = figures_dir / \"variance_normalization_comparison.png\"\n",
    "plt.savefig(variance_comparison_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"  ‚úì Saved: {variance_comparison_path.name}\")\n",
    "plt.close()\n",
    "\n",
    "# Feature-wise comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'variance_before': scaled_variance.values,\n",
    "    'variance_after': new_variance.values,\n",
    "    'improvement': ((scaled_variance.values - new_variance.values) / scaled_variance.values * 100)\n",
    "})\n",
    "\n",
    "comparison_df = comparison_df.sort_values('variance_before', ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "ax.barh(x - width/2, comparison_df['variance_before'], width, \n",
    "        label='Before', alpha=0.8, color='coral')\n",
    "ax.barh(x + width/2, comparison_df['variance_after'], width, \n",
    "        label='After', alpha=0.8, color='skyblue')\n",
    "\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(comparison_df['feature'], fontsize=8)\n",
    "ax.set_xlabel('Variance', fontweight='bold')\n",
    "ax.set_title('Feature Variance: Before vs After Normalization', \n",
    "             fontweight='bold', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3, axis='x')\n",
    "ax.axvline(x=1.0, color='green', linestyle='--', linewidth=2, label='Target')\n",
    "\n",
    "plt.tight_layout()\n",
    "detailed_comparison_path = figures_dir / \"feature_variance_detailed_comparison.png\"\n",
    "plt.savefig(detailed_comparison_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"  ‚úì Saved: {detailed_comparison_path.name}\")\n",
    "plt.close()\n",
    "\n",
    "# =============================================================================\n",
    "# 6. CORRELATION MATRIX COMPARISON\n",
    "# =============================================================================\n",
    "print(\"\\n[6] Checking feature correlations...\")\n",
    "\n",
    "# Identify highly correlated features (redundancy check)\n",
    "corr_matrix = normalized_df.corr().abs()\n",
    "\n",
    "# Find pairs with correlation > 0.8\n",
    "high_corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if corr_matrix.iloc[i, j] > 0.8:\n",
    "            high_corr_pairs.append({\n",
    "                'feature1': corr_matrix.columns[i],\n",
    "                'feature2': corr_matrix.columns[j],\n",
    "                'correlation': corr_matrix.iloc[i, j]\n",
    "            })\n",
    "\n",
    "if len(high_corr_pairs) > 0:\n",
    "    print(f\"\\n  ‚ö†Ô∏è  Found {len(high_corr_pairs)} highly correlated feature pairs (>0.8):\")\n",
    "    for pair in high_corr_pairs[:5]:  # Show top 5\n",
    "        print(f\"    {pair['feature1']} ‚Üî {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "    print(f\"  üí° Consider removing one from each pair to reduce redundancy.\")\n",
    "else:\n",
    "    print(f\"  ‚úÖ No highly correlated features found.\")\n",
    "\n",
    "# =============================================================================\n",
    "# 7. SAVE NORMALIZED FEATURES\n",
    "# =============================================================================\n",
    "print(\"\\n[7] Saving normalized features...\")\n",
    "\n",
    "# Save final features\n",
    "normalized_path = features_dir / \"user_features_normalized.csv\"\n",
    "user_features_normalized.to_csv(normalized_path, index=False)\n",
    "print(f\"  ‚úì Normalized features: {normalized_path.name}\")\n",
    "\n",
    "# Save transformation pipeline\n",
    "pipeline = {\n",
    "    'log_transform_features': log_transform_features,\n",
    "    'power_transform_features': power_transform_features,\n",
    "    'standard_scaler': scaler_std\n",
    "}\n",
    "\n",
    "pipeline_path = models_dir / \"normalization_pipeline.pkl\"\n",
    "with open(pipeline_path, 'wb') as f:\n",
    "    pickle.dump(pipeline, f)\n",
    "print(f\"  ‚úì Pipeline: {pipeline_path.name}\")\n",
    "\n",
    "# Save comparison report\n",
    "comparison_df.to_csv(features_dir / \"variance_comparison_report.csv\", index=False)\n",
    "print(f\"  ‚úì Report: variance_comparison_report.csv\")\n",
    "\n",
    "# =============================================================================\n",
    "# 8. FINAL RECOMMENDATIONS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ FEATURE NORMALIZATION COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"  Variance ratio improvement: {variance_ratio:.2f}x ‚Üí {new_variance_ratio:.2f}x\")\n",
    "print(f\"  Mean centering: {abs(new_mean).max():.4f} (closer to 0 is better)\")\n",
    "print(f\"  Std deviation: {new_std.min():.4f} - {new_std.max():.4f}\")\n",
    "\n",
    "print(f\"\\nüìÅ Files Created:\")\n",
    "print(f\"  {normalized_path}\")\n",
    "print(f\"  {pipeline_path}\")\n",
    "print(f\"  {variance_comparison_path}\")\n",
    "print(f\"  {detailed_comparison_path}\")\n",
    "\n",
    "print(f\"\\nüéØ RECOMMENDATION:\")\n",
    "if new_variance_ratio < 2:\n",
    "    print(f\"  ‚úÖ USE: user_features_normalized.csv for clustering\")\n",
    "    print(f\"  ‚úÖ Features are well-balanced and ready for K-Means/DBSCAN\")\n",
    "elif new_variance_ratio < 3:\n",
    "    print(f\"  ‚ö†Ô∏è  USE: user_features_normalized.csv (acceptable)\")\n",
    "    print(f\"  üí° Consider testing both normalized and original scaled versions\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è  Variance still high. Next steps:\")\n",
    "    print(f\"     1. Remove outliers (filter top 1% of extreme values)\")\n",
    "    print(f\"     2. Try PCA (reduces to uncorrelated components)\")\n",
    "    print(f\"     3. Use distance metrics robust to scale (Mahalanobis)\")\n",
    "\n",
    "print(f\"\\nüí° For K-Means: Use user_features_normalized.csv\")\n",
    "print(f\"üí° For DBSCAN: Test both (DBSCAN is more scale-sensitive)\")\n",
    "print(f\"üí° For Hierarchical: Normalized version is preferred\")\n",
    "\n",
    "# =============================================================================\n",
    "# 9. OPTIONAL: REMOVE REDUNDANT FEATURES\n",
    "# =============================================================================\n",
    "if len(high_corr_pairs) > 0:\n",
    "    print(f\"\\n[OPTIONAL] Removing redundant features...\")\n",
    "    \n",
    "    # Identify features to drop (keep first of each pair)\n",
    "    features_to_drop = set()\n",
    "    for pair in high_corr_pairs:\n",
    "        # Keep feature with higher variance\n",
    "        var1 = new_variance[pair['feature1']]\n",
    "        var2 = new_variance[pair['feature2']]\n",
    "        \n",
    "        if var1 < var2:\n",
    "            features_to_drop.add(pair['feature1'])\n",
    "        else:\n",
    "            features_to_drop.add(pair['feature2'])\n",
    "    \n",
    "    print(f\"  Features to drop (redundant): {features_to_drop}\")\n",
    "    \n",
    "    # Create reduced feature set\n",
    "    features_to_keep = [col for col in feature_cols if col not in features_to_drop]\n",
    "    \n",
    "    user_features_reduced = user_features_normalized[[\"user_id\"] + features_to_keep].copy()\n",
    "    \n",
    "    reduced_path = features_dir / \"user_features_reduced.csv\"\n",
    "    user_features_reduced.to_csv(reduced_path, index=False)\n",
    "    \n",
    "    print(f\"  ‚úì Reduced features: {reduced_path.name}\")\n",
    "    print(f\"  ‚úì Reduced from {len(feature_cols)} ‚Üí {len(features_to_keep)} features\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
